# Sample Sweep Experiments Guide

## Overview

This document describes the sample sweep experiments that reproduce and extend the work from `archive/EFM_experiments/RRG_4CPE`.

## Experiments

### Reproduced from Archive

The original archive experiments tested:
- **Allreduce**: count = [256, 512, 1024, 2048], iterations=10
- **Alltoall**: bytes = [1, 8, 64]
- **FFT3D**: nx=ny=nz = [256, 512, 1024, 2048], npRow=12

With four routing configurations:
- **ECMP_ASP** (now: shortest_path) - Baseline
- **ugal** (now: ugal) - Adaptive routing
- **MD_IT** (now: nexullance_SD) - Single-demand optimization
- **MD_MP_APST4** (now: nexullance_MD) - Multi-demand optimization

### New Sample Sweep Feature

The key enhancement is systematic exploration of the **number of demand samples** used in multi-demand (MD) optimization:

**Sample counts tested**: 1, 2, 4, 8, 16, 32, 64, 128

This allows us to answer:
1. How does MD performance vary with sample count?
2. What is the optimal number of samples for each benchmark/problem size?
3. Is there diminishing returns beyond a certain sample count?
4. Does optimal sample count depend on problem size or traffic pattern?

## Running Experiments

### Quick Test

Test the framework with a small configuration:

```bash
python3.12 test_sample_sweep_framework.py
```

This runs:
- Allreduce with count=256
- All 4 routing methods (shortest_path, ugal, SD, MD)
- MD with 3 sample counts: [1, 4, 8]

**Estimated time**: ~30 minutes

### Full Experiments

Run complete sample sweep for all benchmarks:

```bash
./run_sample_sweep_experiments.sh
```

This will:
1. Run Allreduce with 4 problem sizes × 4 routing methods
2. For MD: Test 8 different sample counts per problem size
3. Run Alltoall with 3 message sizes × 4 routing methods
4. For MD: Test 8 different sample counts per message size
5. Run FFT3D with 4 problem sizes × 4 routing methods
6. For MD: Test 8 different sample counts per problem size

**Total simulations**: ~200 (depending on failures)
**Estimated time**: Several hours (8-12 hours)

### Analyze Results

After experiments complete:

```bash
python3.12 analyze_sample_sweep_results.py
```

This generates:
- **Sample sweep plots**: Show how speedup varies with sample count for each problem size
- **Optimal samples plot**: Shows optimal sample count for each benchmark/size
- **Summary table**: Comprehensive comparison of all methods

## Output Files

### CSV Files

Each experiment generates multiple CSV files:

1. **shortest_path results**: `{benchmark}_{param}_shortest_path_result_{timestamp}.csv`
   - Columns: param_name, sim_time_ms, speedup

2. **ugal results**: `{benchmark}_{param}_ugal_result_{timestamp}.csv`
   - Columns: param_name, sim_time_ms, speedup

3. **nexullance_SD results**: `{benchmark}_{param}_nexullance_SD_result_{timestamp}.csv`
   - Columns: param_name, baseline_sim_time_ms, optimized_sim_time_ms, speedup, improvement_percent

4. **nexullance_MD sample sweep**: `{benchmark}_{param}_nexullance_MD_sample_sweep_{timestamp}.csv`
   - Columns: param_name, num_samples, baseline_sim_time_ms, optimized_sim_time_ms, speedup, improvement_percent

### Plots

Generated by analysis script:

1. **{Benchmark}_sample_sweep_speedup.png**: 
   - Multiple subplots (one per problem size)
   - X-axis: Number of samples (log scale)
   - Y-axis: Speedup vs. baseline
   - Shows horizontal lines for shortest_path, ugal, SD
   - Shows curve for MD with varying samples

2. **optimal_samples_comparison.png**:
   - 3 subplots (one per benchmark)
   - X-axis: Problem size
   - Y-axis: Optimal sample count (log scale)
   - Color: Best speedup achieved
   - Shows relationship between problem size and optimal samples

3. **summary_comparison.csv**:
   - Comprehensive table with all results
   - Includes optimal sample counts and best speedups for MD

## Expected Results

Based on the archive data, we expect:

### Speedup Trends
- **UGAL**: Modest speedup (1.0x - 1.1x), adaptive but conservative
- **SD**: Good speedup (1.15x - 1.30x), simple single-matrix optimization
- **MD**: Best speedup (1.20x - 1.40x), robust multi-matrix optimization

### Sample Count Trends
- **Low sample counts (1-4)**: MD may underperform SD due to insufficient sampling
- **Medium sample counts (4-16)**: Sweet spot, MD reaches optimal performance
- **High sample counts (32-128)**: Diminishing returns, optimization time increases

### Problem Size Effects
- **Small problems**: Fewer samples needed (traffic more uniform)
- **Large problems**: More samples beneficial (traffic more variable)

## Key Questions Answered

1. **Is MD always better than SD?**
   - Not always! With very few samples (1-2), SD may outperform MD
   - MD needs sufficient samples to capture traffic variability

2. **What's the optimal sample count?**
   - Varies by benchmark and problem size
   - Typically in range 4-16 samples
   - Analysis plots will show exact values

3. **Is there a performance ceiling?**
   - Yes, beyond optimal samples, performance plateaus
   - Optimization time continues to increase
   - Cost-benefit trade-off

4. **Should I use SD or MD?**
   - **SD**: Fast, simple, good for uniform traffic
   - **MD**: Slower optimization, better for variable traffic
   - Use analysis to guide decision per workload

## Implementation Notes

### Key Differences from Archive

1. **Cleaner API**: Direct function calls instead of wrapper classes
2. **Unified naming**: SD/MD instead of IT/MP, shortest_path instead of ECMP_ASP
3. **Better tracking**: All simulations logged in simulation_results/
4. **Systematic sweep**: Automated loop over sample counts
5. **Rich analysis**: Multiple visualization and summary tools

### Technical Details

- **Topology**: RRG (Random Regular Graph), V=36, D=5
- **Endpoints**: 108 (36 routers × 3 endpoints/router)
- **Cores**: 432 (108 endpoints × 4 cores/endpoint)
- **Link BW**: 16 Gbps
- **SST Threads**: 8
- **Max Path Length**: 4 (for MD/APST)

### Optimization Methods

- **SD (Single-Demand)**: Uses accumulated demand matrix across all time
  - Fast optimization (~seconds)
  - Assumes stable traffic pattern
  
- **MD (Multi-Demand)**: Uses multiple sampled demand matrices
  - Slower optimization (~seconds to minutes, depends on samples)
  - Robust to traffic variations
  - Uses APST (All Pairs Shortest Trees) algorithm

## Troubleshooting

### Experiments Taking Too Long
- Start with test script first
- Reduce sample counts: test [1, 4, 8] instead of full range
- Run one benchmark at a time

### MD Optimization Fails
- Gurobi solver timeout or memory issues
- Reduce `max_path_length` from 4 to 3
- Reduce `num_demand_samples`

### Missing Results
- Check `simulation_results/` for individual run logs
- Look for error messages in terminal output
- Some configurations may fail - framework continues with remaining tests

## References

- Archive experiments: `archive/EFM_experiments/RRG_4CPE/`
- Nexullance SD implementation: `topoResearch/nexullance/ultility.py::run_nexullance_IT_return_RT()`
- Nexullance MD implementation: `topoResearch/nexullance/ultility.py::run_nexullance_MP_return_RT()`
- SST-Merlin simulator: SST v13+
